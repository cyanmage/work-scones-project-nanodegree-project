{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and monitor a machine learning workflow for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up this notebook\n",
    "\n",
    "Notes about the instance size and kernel setup: this notebook has been tested on\n",
    "\n",
    "1. The `Python 3 (Data Science)` kernel\n",
    "2. The `ml.t3.medium` instance\n",
    "\n",
    "## Data Staging\n",
    "\n",
    "We'll use a sample dataset called CIFAR to simulate the challenges Scones Unlimited are facing in Image Classification. In order to start working with CIFAR we'll need to:\n",
    "\n",
    "1. Extract the data from a hosting service\n",
    "2. Transform it into a usable shape and format\n",
    "3. Load it into a production system\n",
    "\n",
    "In other words, we're going to do some simple ETL!\n",
    "\n",
    "### 1. Extract the data from the hosting service\n",
    "\n",
    "In the cell below, define a function `extract_cifar_data` that extracts python version of the CIFAR-100 dataset. The CIFAR dataaset is open source and generously hosted by the University of Toronto at: https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_cifar_data(url, filename=\"cifar.tar.gz\"):\n",
    "    \"\"\"A function for extracting the CIFAR-100 dataset and storing it as a gzipped file\n",
    "    \n",
    "    Arguments:\n",
    "    url      -- the URL where the dataset is hosted\n",
    "    filename -- the full path where the dataset will be written\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Todo: request the data from the data url\n",
    "    # Hint: use `requests.get` method\n",
    "    r = requests.get(url)\n",
    "    with open(filename, \"wb\") as file_context:\n",
    "        file_context.write(r.content)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out! Run the following cell and check whether a new file `cifar.tar.gz` is created in the file explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_cifar_data(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform the data into a usable shape and format\n",
    "\n",
    "Clearly, distributing the data as a gzipped archive makes sense for the hosting service! It saves on bandwidth, storage, and it's a widely-used archive format. In fact, it's so widely used that the Python community ships a utility for working with them, `tarfile`, as part of its Standard Library. Execute the following cell to decompress your extracted dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"cifar.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new folder `cifar-100-python` should be created, containing `meta`, `test`, and `train` files. These files are `pickles` and the [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html) provides a simple script that can be used to load them. We've adapted the script below for you to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./cifar-100-python/meta\", \"rb\") as f:\n",
    "    dataset_meta = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(\"./cifar-100-python/test\", \"rb\") as f:\n",
    "    dataset_test = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(\"./cifar-100-python/train\", \"rb\") as f:\n",
    "    dataset_train = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to explore the datasets\n",
    "\n",
    "dataset_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train[b'filenames']), len(dataset_train[b'fine_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[b'data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255, ...,  10,  59,  79],\n",
       "       [255, 253, 253, ..., 253, 253, 255],\n",
       "       [250, 248, 247, ..., 194, 207, 228],\n",
       "       [124, 131, 135, ..., 232, 236, 231],\n",
       "       [ 43,  32,  87, ...,  60,  29,  37]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[b'data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As documented on the homepage, `b'data'` contains rows of 3073 unsigned integers, representing three channels (red, green, and blue) for one 32x32 pixel image per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple gut-check, let's transform one of our images. Each 1024 items in a row is a channel (red, green, then blue). Each 32 items in the channel are a row in the 32x32 image. Using python, we can stack these channels into a 32x32x3 array, and save it as a PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each 1024 in a row is a channel (red, green, then blue)\n",
    "row = dataset_train[b'data'][0]\n",
    "red, green, blue = row[0:1024], row[1024:2048], row[2048:]\n",
    "\n",
    "# Each 32 items in the channel are a row in the 32x32 image\n",
    "red = red.reshape(32,32)\n",
    "green = green.reshape(32,32)\n",
    "blue = blue.reshape(32,32)\n",
    "\n",
    "# Combine the channels into a 32x32x3 image!\n",
    "combined = np.dstack((red,green,blue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more concise version, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All in one:\n",
    "test_image = np.dstack((\n",
    "    row[0:1024].reshape(32,32),\n",
    "    row[1024:2048].reshape(32,32),\n",
    "    row[2048:].reshape(32,32)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a cow! Let's check the label. `dataset_meta` contains label names in order, and `dataset_train` has a list of labels for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_meta[b'fine_label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[b'fine_labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image has a label of `19`, so let's see what the 19th item is in the list of label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'cattle'\n"
     ]
    }
   ],
   "source": [
    "print(dataset_meta[b'fine_label_names'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! 'cattle' sounds about right. By the way, using the previous two lines we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'cattle'\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print(dataset_meta[b'fine_label_names'][dataset_train[b'fine_labels'][n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to check labels, is there a way that we can also check file names? `dataset_train` also contains a `b'filenames'` key. Let's see what we have here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'bos_taurus_s_000507.png'\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[b'filenames'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Taurus\" is the name of a subspecies of cattle, so this looks like a pretty reasonable filename. To save an image we can also do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"file.png\", test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your new PNG file should now appear in the file explorer -- go ahead and pop it open to see!\n",
    "\n",
    "Now that you know how to reshape the images, save them as files, and capture their filenames and labels, let's just capture all the bicycles and motorcycles and save them. Scones Unlimited can use a model that tells these apart to route delivery drivers automatically.\n",
    "\n",
    "In the following cell, identify the label numbers for Bicycles and Motorcycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 48)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Todo: Filter the dataset_train and dataset_meta objects to find the label numbers for Bicycle and Motorcycles\n",
    "\n",
    "### TOO \"COMPLICATED\"\n",
    "#list(filter(lambda x: dataset_meta[b'fine_label_names'][x]==b'motorcycle', range(len(dataset_meta[b'fine_label_names']))))\n",
    "####   b'bicycle'          b'motorcycle'\n",
    "\n",
    "idx_motorcycle, idx_bicycle = 0, 0\n",
    "while dataset_meta[b'fine_label_names'][idx_bicycle] != b'bicycle':\n",
    "    idx_bicycle += 1\n",
    "while dataset_meta[b'fine_label_names'][idx_motorcycle] != b'motorcycle':\n",
    "    idx_motorcycle += 1\n",
    "    \n",
    "idx_bicycle, idx_motorcycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! We only need objects with label 8 and 48 -- this drastically simplifies our handling of the data! Below we construct a dataframe for you, and you can safely drop the rows that don't contain observations about bicycles and motorcycles. Fill in the missing lines below to drop all other rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the dataframe\n",
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_train where label is not 8 or 48\n",
    "df_train = df_train[df_train['labels'].isin([8, 48])]\n",
    "\n",
    "# Decode df_train.filenames so they are regular strings\n",
    "df_train[\"filenames\"] = df_train[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_test where label is not 8 or 48\n",
    "df_test = df_test[df_test['labels'].isin([8, 48])]\n",
    "\n",
    "# Decode df_test.filenames so they are regular strings\n",
    "df_test[\"filenames\"] = df_test[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>labels</th>\n",
       "      <th>row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bike_s_000682.png</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bike_s_000127.png</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>cycle_s_002598.png</td>\n",
       "      <td>8</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>minibike_s_000824.png</td>\n",
       "      <td>48</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>motorcycle_s_001856.png</td>\n",
       "      <td>48</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>bicycle_s_000537.png</td>\n",
       "      <td>8</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>moped_s_000169.png</td>\n",
       "      <td>48</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>motorcycle_s_001453.png</td>\n",
       "      <td>48</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>safety_bike_s_000482.png</td>\n",
       "      <td>8</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>dirt_bike_s_000124.png</td>\n",
       "      <td>48</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>minibike_s_001863.png</td>\n",
       "      <td>48</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>minibike_s_001177.png</td>\n",
       "      <td>48</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>motorcycle_s_001792.png</td>\n",
       "      <td>48</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>minibike_s_001969.png</td>\n",
       "      <td>48</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>velocipede_s_001585.png</td>\n",
       "      <td>8</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filenames  labels  row\n",
       "16          bike_s_000682.png       8   16\n",
       "30          bike_s_000127.png       8   30\n",
       "130        cycle_s_002598.png       8  130\n",
       "152     minibike_s_000824.png      48  152\n",
       "195   motorcycle_s_001856.png      48  195\n",
       "219      bicycle_s_000537.png       8  219\n",
       "251        moped_s_000169.png      48  251\n",
       "252   motorcycle_s_001453.png      48  252\n",
       "298  safety_bike_s_000482.png       8  298\n",
       "370    dirt_bike_s_000124.png      48  370\n",
       "390     minibike_s_001863.png      48  390\n",
       "392     minibike_s_001177.png      48  392\n",
       "408   motorcycle_s_001792.png      48  408\n",
       "560     minibike_s_001969.png      48  560\n",
       "575   velocipede_s_001585.png       8  575"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 48]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_train['labels'] == 8).sum(), (df_train['labels'] == 48).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>labels</th>\n",
       "      <th>row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>safety_bike_s_000390.png</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bike_s_000658.png</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>velocipede_s_001744.png</td>\n",
       "      <td>8</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>bike_s_000643.png</td>\n",
       "      <td>8</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ordinary_bicycle_s_000437.png</td>\n",
       "      <td>8</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>motorcycle_s_000323.png</td>\n",
       "      <td>48</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>safety_bike_s_001253.png</td>\n",
       "      <td>8</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>cycle_s_001306.png</td>\n",
       "      <td>8</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>velocipede_s_000863.png</td>\n",
       "      <td>8</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>bicycle_s_000479.png</td>\n",
       "      <td>8</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>minibike_s_002173.png</td>\n",
       "      <td>48</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>motorcycle_s_000352.png</td>\n",
       "      <td>48</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>bike_s_000801.png</td>\n",
       "      <td>8</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>motorbike_s_000433.png</td>\n",
       "      <td>48</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>bike_s_001068.png</td>\n",
       "      <td>8</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filenames  labels  row\n",
       "27        safety_bike_s_000390.png       8   27\n",
       "28               bike_s_000658.png       8   28\n",
       "116        velocipede_s_001744.png       8  116\n",
       "161              bike_s_000643.png       8  161\n",
       "319  ordinary_bicycle_s_000437.png       8  319\n",
       "354        motorcycle_s_000323.png      48  354\n",
       "417       safety_bike_s_001253.png       8  417\n",
       "476             cycle_s_001306.png       8  476\n",
       "532        velocipede_s_000863.png       8  532\n",
       "554           bicycle_s_000479.png       8  554\n",
       "615          minibike_s_002173.png      48  615\n",
       "626        motorcycle_s_000352.png      48  626\n",
       "644              bike_s_000801.png       8  644\n",
       "752         motorbike_s_000433.png      48  752\n",
       "756              bike_s_001068.png       8  756"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_test['labels'] == 8).sum(), (df_test['labels'] == 48).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 48]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_test['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (200, 3))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is filtered for just our classes, we can save all our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./train/images\n",
    "!mkdir -p ./test/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections we introduced you to several key snippets of code:\n",
    "\n",
    "1. Grabbing the image data:\n",
    "\n",
    "```python\n",
    "dataset_train[b'data'][0]\n",
    "```\n",
    "\n",
    "2. A simple idiom for stacking the image data into the right shape\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.dstack((\n",
    "    row[0:1024].reshape(32,32),\n",
    "    row[1024:2048].reshape(32,32),\n",
    "    row[2048:].reshape(32,32)\n",
    "))\n",
    "```\n",
    "\n",
    "3. A simple `matplotlib` utility for saving images\n",
    "\n",
    "```python\n",
    "plt.imsave(path+row['filenames'], target)\n",
    "```\n",
    "\n",
    "------------ CHANGED TO ADAPT MY ARBORESCENCE   -------------\n",
    "\n",
    "Compose these together into a function that saves all the images into the `./test/images` and `./train/images` directories. Use the comments in the body of the `save_images` function below to guide your construction of the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_images(idx_image, channel, dataset):\n",
    "    #Grab the image data in row-major form\n",
    "    img = dataset[b'data'][idx_image]\n",
    "    \n",
    "    # Consolidated stacking/reshaping from earlier\n",
    "    target = np.dstack((\n",
    "            img[:1024].reshape(32, 32),\n",
    "            img[1024:2048].reshape(32, 32),\n",
    "            img[2048:].reshape(32, 32)\n",
    "    ))\n",
    "    \n",
    "    # Save the image\n",
    "    plt.imsave(f\"./{channel}/images/{dataset[b'filenames'][idx_image].decode('utf-8')}\", target)\n",
    "    \n",
    "    # Return any signal data you want for debugging\n",
    "    return\n",
    "\n",
    "## TODO: save ALL images using the save_images function\n",
    "\n",
    "number_images_test = dataset_test[b'data'].shape[0]\n",
    "number_images_train = dataset_train[b'data'].shape[0]\n",
    "\n",
    "#[save_images(index, 'train', dataset_train) for index in range(number_images_train)]\n",
    "for idx in range(number_images_test):\n",
    "    save_images(idx, 'test', dataset_test)\n",
    "\n",
    "#[save_images(index, 'test', dataset_test) for index in range(number_images_test)]\n",
    "for idx in range(number_images_train):\n",
    "    save_images(idx, 'train', dataset_train)\n",
    "    \n",
    "number_images_train, number_images_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the data\n",
    "\n",
    "Now we can load the data into S3.\n",
    "\n",
    "Using the sagemaker SDK grab the current region, execution role, and bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sagemaker-us-east-1-416312206633',\n",
       " 'us-east-1',\n",
       " 'arn:aws:iam::416312206633:role/service-role/AmazonSageMaker-ExecutionRole-20211216T204468')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.default_bucket(), sess.boto_region_name, get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Bucket: scones-project-working\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::416312206633:role/service-role/AmazonSageMaker-ExecutionRole-20211216T204468\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket= 'scones-project-working'\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = sess.boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can easily sync your data up into S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket\n",
    "!aws s3 sync ./train s3://${DEFAULT_S3_BUCKET}/train/\n",
    "!aws s3 sync ./test s3://${DEFAULT_S3_BUCKET}/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You can check the bucket and verify that the items were uploaded.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "For Image Classification, Sagemaker [also expects metadata](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html) e.g. in the form of TSV files with labels and filepaths. We can generate these using our Pandas DataFrames from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_metadata_file(df, prefix):\n",
    "    df[\"s3_path\"] = df[\"filenames\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply(lambda x: 0 if x==8 else 1)\n",
    "    return df[[\"row\", \"labels\", \"s3_path\"]].to_csv(\n",
    "        f\"{prefix}.lst\", sep=\"\\t\", index=False, header=False\n",
    "    )\n",
    "    \n",
    "to_metadata_file(df_train.copy(), \"train\")\n",
    "to_metadata_file(df_test.copy(), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also upload our manifest files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Upload files\n",
    "boto3.Session().resource('s3').Bucket(\n",
    "    bucket).Object('train.lst').upload_file('./train.lst')\n",
    "boto3.Session().resource('s3').Bucket(\n",
    "    bucket).Object('test.lst').upload_file('./test.lst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `bucket` and `region` info we can get the latest prebuilt container to run our training job, and define an output location on our s3 bucket for the model. Use the `image_uris` function from the SageMaker SDK to retrieve the latest `image-classification` image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Use the image_uris function to retrieve the latest 'image-classification' image \n",
    "algo_image = image_uris.retrieve(framework='image-classification', region=region)\n",
    "s3_output_location = f\"s3://{bucket}/models/image_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://scones-project-working/models/image_model'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_output_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to create an estimator! Create an estimator `img_classifier_model` that uses one instance of `ml.p2.xlarge`. Ensure that y ou use the output location we defined above - we'll be referring to that later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier_model=sagemaker.estimator.Estimator(\n",
    "    ## TODO: define your estimator options  \n",
    "    algo_image,\n",
    "    role,\n",
    "    instance_count= 1,\n",
    "    instance_type= 'ml.p2.xlarge',\n",
    "    #use_spot_instances= True,\n",
    "    output_path= s3_output_location,\n",
    "    #max_wait = 3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a few key hyperparameters and define the inputs for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier_model.set_hyperparameters(\n",
    "    image_shape= '3,32,32', # TODO: Fill in\n",
    "    num_classes=2, # TODO: Fill in\n",
    "    num_training_samples=1000 # TODO: fill in\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image-classification` image uses four input channels with very specific input parameters. For convenience, we've provided them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "model_inputs = {\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/train/images/\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/test/images/\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"train_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/train.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/test.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can train the model using the model_inputs. In the cell below, call the `fit` method on our model,:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-25 20:16:46 Starting - Starting the training job...\n",
      "2021-12-25 20:17:09 Starting - Launching requested ML instancesProfilerReport-1640463406: InProgress\n",
      "......\n",
      "2021-12-25 20:18:09 Starting - Preparing the instances for training.........\n",
      "2021-12-25 20:19:30 Downloading - Downloading input data..........................................\n",
      "2021-12-25 20:26:35 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'num_classes': '2', 'num_training_samples': '1000', 'image_shape': '3,32,32'}\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Final configuration: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,32,32', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '1000'}\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Creating record files for train.lst\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Done creating record files...\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Creating record files for test.lst\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Done creating record files...\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] multi_label: 0\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] num_layers: 152\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] epochs: 30\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] num_training_samples: 1000\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] image_shape: 3,32,32\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] num_classes: 2\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] kv_store: device\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] --------------------\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:04 INFO 139644080904000] Setting number of threads: 3\u001b[0m\n",
      "\n",
      "2021-12-25 20:27:12 Training - Training image download completed. Training in progress.\u001b[34m[20:27:13] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.10042.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:32 INFO 139644080904000] Epoch[0] Batch [20]#011Speed: 34.910 samples/sec#011accuracy=0.525298\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:37 INFO 139644080904000] Epoch[0] Train-accuracy=0.591734\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:37 INFO 139644080904000] Epoch[0] Time cost=23.703\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:38 INFO 139644080904000] Epoch[0] Validation-accuracy=0.510417\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:39 INFO 139644080904000] Storing the best model with validation accuracy: 0.510417\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:39 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:50 INFO 139644080904000] Epoch[1] Batch [20]#011Speed: 58.657 samples/sec#011accuracy=0.711310\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:55 INFO 139644080904000] Epoch[1] Train-accuracy=0.721774\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:55 INFO 139644080904000] Epoch[1] Time cost=16.287\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:27:57 INFO 139644080904000] Epoch[1] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:08 INFO 139644080904000] Epoch[2] Batch [20]#011Speed: 58.068 samples/sec#011accuracy=0.721726\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:14 INFO 139644080904000] Epoch[2] Train-accuracy=0.718750\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:14 INFO 139644080904000] Epoch[2] Time cost=16.529\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:15 INFO 139644080904000] Epoch[2] Validation-accuracy=0.682292\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:16 INFO 139644080904000] Storing the best model with validation accuracy: 0.682292\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:16 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:27 INFO 139644080904000] Epoch[3] Batch [20]#011Speed: 58.528 samples/sec#011accuracy=0.755952\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:32 INFO 139644080904000] Epoch[3] Train-accuracy=0.773185\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:32 INFO 139644080904000] Epoch[3] Time cost=16.333\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:34 INFO 139644080904000] Epoch[3] Validation-accuracy=0.647321\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:46 INFO 139644080904000] Epoch[4] Batch [20]#011Speed: 58.259 samples/sec#011accuracy=0.738095\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:51 INFO 139644080904000] Epoch[4] Train-accuracy=0.747984\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:51 INFO 139644080904000] Epoch[4] Time cost=16.416\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:52 INFO 139644080904000] Epoch[4] Validation-accuracy=0.791667\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:53 INFO 139644080904000] Storing the best model with validation accuracy: 0.791667\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:28:53 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:04 INFO 139644080904000] Epoch[5] Batch [20]#011Speed: 57.924 samples/sec#011accuracy=0.788690\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:10 INFO 139644080904000] Epoch[5] Train-accuracy=0.778226\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:10 INFO 139644080904000] Epoch[5] Time cost=16.495\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:11 INFO 139644080904000] Epoch[5] Validation-accuracy=0.770833\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:23 INFO 139644080904000] Epoch[6] Batch [20]#011Speed: 58.402 samples/sec#011accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:29 INFO 139644080904000] Epoch[6] Train-accuracy=0.789315\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:29 INFO 139644080904000] Epoch[6] Time cost=17.014\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:30 INFO 139644080904000] Epoch[6] Validation-accuracy=0.744792\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:42 INFO 139644080904000] Epoch[7] Batch [20]#011Speed: 58.402 samples/sec#011accuracy=0.755952\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:47 INFO 139644080904000] Epoch[7] Train-accuracy=0.763105\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:47 INFO 139644080904000] Epoch[7] Time cost=16.353\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:29:49 INFO 139644080904000] Epoch[7] Validation-accuracy=0.758929\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:00 INFO 139644080904000] Epoch[8] Batch [20]#011Speed: 58.171 samples/sec#011accuracy=0.788690\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:06 INFO 139644080904000] Epoch[8] Train-accuracy=0.791331\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:06 INFO 139644080904000] Epoch[8] Time cost=16.454\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:07 INFO 139644080904000] Epoch[8] Validation-accuracy=0.828125\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:07 INFO 139644080904000] Storing the best model with validation accuracy: 0.828125\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:08 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0009.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:19 INFO 139644080904000] Epoch[9] Batch [20]#011Speed: 58.351 samples/sec#011accuracy=0.827381\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:24 INFO 139644080904000] Epoch[9] Train-accuracy=0.830645\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:24 INFO 139644080904000] Epoch[9] Time cost=16.358\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:26 INFO 139644080904000] Epoch[9] Validation-accuracy=0.744792\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:37 INFO 139644080904000] Epoch[10] Batch [20]#011Speed: 58.462 samples/sec#011accuracy=0.828869\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:43 INFO 139644080904000] Epoch[10] Train-accuracy=0.819556\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:43 INFO 139644080904000] Epoch[10] Time cost=16.443\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:44 INFO 139644080904000] Epoch[10] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:30:56 INFO 139644080904000] Epoch[11] Batch [20]#011Speed: 58.253 samples/sec#011accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:01 INFO 139644080904000] Epoch[11] Train-accuracy=0.819556\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:01 INFO 139644080904000] Epoch[11] Time cost=16.387\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:03 INFO 139644080904000] Epoch[11] Validation-accuracy=0.816964\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:14 INFO 139644080904000] Epoch[12] Batch [20]#011Speed: 57.825 samples/sec#011accuracy=0.837798\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:20 INFO 139644080904000] Epoch[12] Train-accuracy=0.843750\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:20 INFO 139644080904000] Epoch[12] Time cost=16.460\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:21 INFO 139644080904000] Epoch[12] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:32 INFO 139644080904000] Epoch[13] Batch [20]#011Speed: 58.317 samples/sec#011accuracy=0.828869\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:38 INFO 139644080904000] Epoch[13] Train-accuracy=0.829637\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:38 INFO 139644080904000] Epoch[13] Time cost=16.357\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:39 INFO 139644080904000] Epoch[13] Validation-accuracy=0.854167\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:40 INFO 139644080904000] Storing the best model with validation accuracy: 0.854167\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:40 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0014.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:51 INFO 139644080904000] Epoch[14] Batch [20]#011Speed: 58.437 samples/sec#011accuracy=0.866071\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:56 INFO 139644080904000] Epoch[14] Train-accuracy=0.873992\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:56 INFO 139644080904000] Epoch[14] Time cost=16.346\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:31:58 INFO 139644080904000] Epoch[14] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:09 INFO 139644080904000] Epoch[15] Batch [20]#011Speed: 57.770 samples/sec#011accuracy=0.870536\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:15 INFO 139644080904000] Epoch[15] Train-accuracy=0.862903\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:15 INFO 139644080904000] Epoch[15] Time cost=16.562\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:16 INFO 139644080904000] Epoch[15] Validation-accuracy=0.794643\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:28 INFO 139644080904000] Epoch[16] Batch [20]#011Speed: 57.356 samples/sec#011accuracy=0.888393\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:34 INFO 139644080904000] Epoch[16] Train-accuracy=0.867944\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:34 INFO 139644080904000] Epoch[16] Time cost=16.573\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:35 INFO 139644080904000] Epoch[16] Validation-accuracy=0.781250\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:47 INFO 139644080904000] Epoch[17] Batch [20]#011Speed: 58.275 samples/sec#011accuracy=0.875000\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:52 INFO 139644080904000] Epoch[17] Train-accuracy=0.872984\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:52 INFO 139644080904000] Epoch[17] Time cost=16.386\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:32:53 INFO 139644080904000] Epoch[17] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:05 INFO 139644080904000] Epoch[18] Batch [20]#011Speed: 58.021 samples/sec#011accuracy=0.854167\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:10 INFO 139644080904000] Epoch[18] Train-accuracy=0.866935\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:10 INFO 139644080904000] Epoch[18] Time cost=16.497\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:12 INFO 139644080904000] Epoch[18] Validation-accuracy=0.828125\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:23 INFO 139644080904000] Epoch[19] Batch [20]#011Speed: 58.330 samples/sec#011accuracy=0.913690\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:29 INFO 139644080904000] Epoch[19] Train-accuracy=0.911290\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:29 INFO 139644080904000] Epoch[19] Time cost=16.393\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:30 INFO 139644080904000] Epoch[19] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:42 INFO 139644080904000] Epoch[20] Batch [20]#011Speed: 58.071 samples/sec#011accuracy=0.928571\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:47 INFO 139644080904000] Epoch[20] Train-accuracy=0.920363\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:47 INFO 139644080904000] Epoch[20] Time cost=16.440\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:33:49 INFO 139644080904000] Epoch[20] Validation-accuracy=0.838542\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:00 INFO 139644080904000] Epoch[21] Batch [20]#011Speed: 58.205 samples/sec#011accuracy=0.922619\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:06 INFO 139644080904000] Epoch[21] Train-accuracy=0.922379\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:06 INFO 139644080904000] Epoch[21] Time cost=16.505\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:07 INFO 139644080904000] Epoch[21] Validation-accuracy=0.833333\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:19 INFO 139644080904000] Epoch[22] Batch [20]#011Speed: 58.241 samples/sec#011accuracy=0.933036\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:24 INFO 139644080904000] Epoch[22] Train-accuracy=0.921371\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:24 INFO 139644080904000] Epoch[22] Time cost=16.382\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:26 INFO 139644080904000] Epoch[22] Validation-accuracy=0.859375\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:26 INFO 139644080904000] Storing the best model with validation accuracy: 0.859375\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:26 INFO 139644080904000] Saved checkpoint to \"/opt/ml/model/image-classification-0023.params\"\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:37 INFO 139644080904000] Epoch[23] Batch [20]#011Speed: 58.163 samples/sec#011accuracy=0.938988\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:43 INFO 139644080904000] Epoch[23] Train-accuracy=0.931452\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:43 INFO 139644080904000] Epoch[23] Time cost=16.409\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:44 INFO 139644080904000] Epoch[23] Validation-accuracy=0.830357\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:34:56 INFO 139644080904000] Epoch[24] Batch [20]#011Speed: 57.601 samples/sec#011accuracy=0.941964\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:01 INFO 139644080904000] Epoch[24] Train-accuracy=0.950605\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:01 INFO 139644080904000] Epoch[24] Time cost=16.533\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:03 INFO 139644080904000] Epoch[24] Validation-accuracy=0.833333\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:15 INFO 139644080904000] Epoch[25] Batch [20]#011Speed: 57.550 samples/sec#011accuracy=0.955357\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:20 INFO 139644080904000] Epoch[25] Train-accuracy=0.956653\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:20 INFO 139644080904000] Epoch[25] Time cost=16.520\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:21 INFO 139644080904000] Epoch[25] Validation-accuracy=0.802083\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:33 INFO 139644080904000] Epoch[26] Batch [20]#011Speed: 58.330 samples/sec#011accuracy=0.915179\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:38 INFO 139644080904000] Epoch[26] Train-accuracy=0.923387\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:38 INFO 139644080904000] Epoch[26] Time cost=16.382\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:40 INFO 139644080904000] Epoch[26] Validation-accuracy=0.848958\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:51 INFO 139644080904000] Epoch[27] Batch [20]#011Speed: 58.209 samples/sec#011accuracy=0.974702\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:57 INFO 139644080904000] Epoch[27] Train-accuracy=0.968750\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:57 INFO 139644080904000] Epoch[27] Time cost=16.418\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:35:58 INFO 139644080904000] Epoch[27] Validation-accuracy=0.821429\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:10 INFO 139644080904000] Epoch[28] Batch [20]#011Speed: 57.655 samples/sec#011accuracy=0.961310\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:15 INFO 139644080904000] Epoch[28] Train-accuracy=0.957661\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:15 INFO 139644080904000] Epoch[28] Time cost=16.502\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:17 INFO 139644080904000] Epoch[28] Validation-accuracy=0.807292\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:28 INFO 139644080904000] Epoch[29] Batch [20]#011Speed: 58.475 samples/sec#011accuracy=0.965774\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:34 INFO 139644080904000] Epoch[29] Train-accuracy=0.966734\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:34 INFO 139644080904000] Epoch[29] Time cost=16.341\u001b[0m\n",
      "\u001b[34m[12/25/2021 20:36:35 INFO 139644080904000] Epoch[29] Validation-accuracy=0.817708\u001b[0m\n",
      "\n",
      "2021-12-25 20:37:01 Uploading - Uploading generated training model\n",
      "2021-12-25 20:37:56 Completed - Training job completed\n",
      "ProfilerReport-1640463406: IssuesFound\n",
      "Training seconds: 1099\n",
      "Billable seconds: 1099\n"
     ]
    }
   ],
   "source": [
    "## TODO: train your model\n",
    "img_classifier_model.fit(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you'll end up with a model topping out above `.8` validation accuracy. With only 1000 training samples in the CIFAR dataset, that's pretty good. We could definitely pursue data augmentation & gathering more samples to help us improve further, but for now let's proceed to deploy our model.\n",
    "\n",
    "### Getting ready to deploy\n",
    "\n",
    "To begin with, let's configure Model Monitor to track our deployment. We'll define a `DataCaptureConfig` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    ## TODO: Set config options\n",
    "    enable_capture= True,\n",
    "    sampling_percentage= 100,\n",
    "    destination_s3_uri= f\"s3://{bucket}/data_capture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `destination_s3_uri` parameter: At the end of the project, we can explore the `data_capture` directory in S3 to find crucial data about the inputs and outputs Model Monitor has observed on our model endpoint over time.\n",
    "\n",
    "With that done, deploy your model on a single `ml.m5.xlarge` instance with the data capture config attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!image-classification-2021-12-25-21-01-01-449\n"
     ]
    }
   ],
   "source": [
    "deployment = img_classifier_model.deploy(\n",
    "    ## TODO: fill in deployment options\n",
    "    data_capture_config=data_capture_config,\n",
    "    initial_instance_count= 1, \n",
    "    instance_type= 'ml.m5.xlarge'\n",
    "    )\n",
    "\n",
    "endpoint = deployment.endpoint_name\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the endpoint name for later as well.\n",
    "\n",
    "Next, instantiate a Predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "## TODO: fill in\n",
    "predictor = Predictor(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below we are going to prepare one of your saved images for prediction. Use the predictor to process the `payload`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "import base64\n",
    "\n",
    "predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "with open(\"./test/images/bicycle_s_001789.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "## TODO: Process the payload with your predictor    \n",
    "inference = predictor.predict(payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `inference` object is an array of two values, the predicted probability value for each of your classes (bicycle and motorcycle respectively.) So, for example, a value of `b'[0.91, 0.09]'` indicates the probability of being a bike is 91% and being a motorcycle is 9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[0.99795001745224, 0.002049974864348769]'\n"
     ]
    }
   ],
   "source": [
    "print(inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft Lambdas and Step Function Workflow\n",
    "\n",
    "Your operations team uses Step Functions to orchestrate serverless workflows. One of the nice things about Step Functions is that [workflows can call other workflows](https://docs.aws.amazon.com/step-functions/latest/dg/connect-stepfunctions.html), so the team can easily plug your workflow into the broader production architecture for Scones Unlimited.\n",
    "\n",
    "In this next stage you're going to write and deploy three Lambda functions, and then use the Step Functions visual editor to chain them together! Our functions are going to work with a simple data object:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"inferences\": [], # Output of predictor.predict\n",
    "    \"s3_key\": \"\", # Source data S3 key\n",
    "    \"s3_bucket\": \"\", # Source data S3 bucket\n",
    "    \"image_data\": \"\"  # base64 encoded string containing the image data\n",
    "}\n",
    "```\n",
    "\n",
    "A good test object that you can use for Lambda tests and Step Function executions, throughout the next section, might look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"image_data\": \"\",\n",
    "  \"s3_bucket\": \"scones-project-working\", # Fill in with your bucket\n",
    "  \"s3_key\": \"test/images/bicycle_s_000513.png\"\n",
    "}\n",
    "```\n",
    "\n",
    "Using these fields, your functions can read and write the necessary data to execute your workflow. Let's start with the first function. Your first Lambda function will copy an object from S3, base64 encode it, and then return it to the step function as `image_data` in an event.\n",
    "\n",
    "Go to the Lambda dashboard and create a new Lambda function with a descriptive name like \"serializeImageData\" and select thr 'Python 3.8' runtime. Add the same permissions as the SageMaker role you created earlier. (Reminder: you do this in the Configuration tab under \"Permissions\"). Once you're ready, use the starter code below to craft your Lambda handler:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"A function to serialize target data from S3\"\"\"\n",
    "    \n",
    "    # Get the s3 address from the Step Function event input\n",
    "    bucket = event['bucket_s3']## TODO: fill in\n",
    "    key = event['key_s3']## TODO: fill in\n",
    "    \n",
    "    # Download the data from s3 to /tmp/image.png\n",
    "    ## TODO: fill in\n",
    "    s3.download_file(bucket, key, '/tmp/image.png')\n",
    "    \n",
    "    # We read the data from a file\n",
    "    with open(\"/tmp/image.png\", \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read())\n",
    "\n",
    "    # Pass the data back to the Step Function\n",
    "    print(\"Event:\", event.keys())\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            \"image_data\": image_data,\n",
    "            \"s3_bucket\": bucket,\n",
    "            \"s3_key\": key,\n",
    "            \"inferences\": []\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "The next function is responsible for the classification part - we're going to take the image output from the previous function, decode it, and then pass inferences back to the the Step Function.\n",
    "\n",
    "Because this Lambda will have runtime dependencies (i.e. the SageMaker SDK) you'll need to package them in your function. *Key reading:* https://docs.aws.amazon.com/lambda/latest/dg/python-package-create.html#python-package-create-with-dependency\n",
    "\n",
    "Create a new Lambda function with the same rights and a descriptive name, then fill in the starter code below for your classifier Lambda.\n",
    "\n",
    "```python\n",
    "import json\n",
    "#import sagemaker\n",
    "import base64\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "\n",
    "# Fill this in with the name of your deployed model\n",
    "ENDPOINT = 'scone-endpoint' ## TODO: fill in\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    #    print(f' --------- INDICE --------- / {event.keys()}')\n",
    "    \n",
    "    # Decode the image data\n",
    "    image = base64.b64decode(event['image_data']) ## TODO: fill in\n",
    "\n",
    "\n",
    "    ###########             NOT USED           ##############\n",
    "    # Instantiate a Predictor ----> NOT USED\n",
    "    # predictor = Predictor(ENDPOINT) \n",
    "    #\n",
    "    # NEED A LINUX MACHINE OR A VM LINUX IN WINDOWS, OR CLOUD9+CLI TO PACKAGE SAGEMAKER  \n",
    "    # LOCALLY ( WITH \"LINUX DEPENDENCIES\") TO BE UPLOADED TO LAMBDA\n",
    "    #\n",
    "    # -->> better fastly use a solution found in forum : https://knowledge.udacity.com/questions/767689\n",
    "    # Use of 'runtime.sagemaker' client in boto3, imported natively, rather than importing sagemaker\n",
    "    \n",
    "    \n",
    "    # For this model the IdentitySerializer needs to be \"image/png\"\n",
    "    # predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "\n",
    "\n",
    "\n",
    "    # Make a prediction:\n",
    "    #inferences = predictor.predict(event['s3_bucket'] + event['s3_key'])## TODO: fill in\n",
    "    ############################################################   \n",
    "    \n",
    "    # print(f'ENDPOINT: {ENDPOINT}')\n",
    "\n",
    "    response = client.invoke_endpoint( EndpointName = ENDPOINT, ContentType = 'image/png', Body=image)\n",
    "\n",
    "    # Make a prediction:\n",
    "    inferences = response['Body'].read()\n",
    "    \n",
    "    # We return the data back to the Step Function    \n",
    "    event['inferences'] = inferences.decode('utf-8')\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event)\n",
    "    }\n",
    "```\n",
    "\n",
    "Finally, we need to filter low-confidence inferences. Define a threshold between 1.00 and 0.000 for your model: what is reasonble for you? If the model predicts at `.70` for it's highest confidence label, do we want to pass that inference along to downstream systems? Make one last Lambda function and tee up the same permissions:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "\n",
    "THRESHOLD = .93\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    print(event)\n",
    "    # Grab the inferences from the event\n",
    "    inferences = json.loads(event['body'])['inferences']     ## TODO: fill in\n",
    "    inferences = json.loads(inferences)\n",
    "    print(f'INFERENCES : {inferences}')\n",
    "    \n",
    "    # Check if any values in our inferences are above THRESHOLD\n",
    "    meets_threshold = (inferences[0]>THRESHOLD or inferences[1]>THRESHOLD)   ## TODO: fill in\n",
    "    \n",
    "    # If our threshold is met, pass our data back out of the\n",
    "    # Step Function, else, end the Step Function with an error\n",
    "    if meets_threshold:\n",
    "        print(\"THRESHOLD MET\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"!!!!!!!     THRESHOLD NOT MET       !!!!!!\")\n",
    "        raise(\"THRESHOLD_CONFIDENCE_NOT_MET\")\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event),\n",
    "        'meets_threshold' : meets_threshold\n",
    "    }\n",
    "```\n",
    "Once you have tested the lambda functions, save the code for each lambda function in a python script called 'lambda.py'.\n",
    "\n",
    "With your lambdas in place, you can use the Step Functions visual editor to construct a workflow that chains them together. In the Step Functions console you'll have the option to author a Standard step function *Visually*.\n",
    "\n",
    "When the visual editor opens, you'll have many options to add transitions in your workflow. We're going to keep it simple and have just one: to invoke Lambda functions. Add three of them chained together. For each one, you'll be able to select the Lambda functions you just created in the proper order, filter inputs and outputs, and give them descriptive names.\n",
    "\n",
    "Make sure that you:\n",
    "\n",
    "1. Are properly filtering the inputs and outputs of your invokations (e.g. `$.body`)\n",
    "2. Take care to remove the error handling from the last function - it's supposed to \"fail loudly\" for your operations colleagues!\n",
    "\n",
    "Take a screenshot of your working step function in action and export the step function as JSON for your submission package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! Now you can use the files in `./test` as test files for our workflow. Depending on our threshold, our workflow should reliably pass predictions about images from `./test` on to downstream systems, while erroring out for inferences below our confidence threshold!\n",
    "\n",
    "### Testing and Evaluation\n",
    "\n",
    "Do several step function invokations using data from the `./test` folder. This process should give you confidence that the workflow both *succeeds* AND *fails* as expected. In addition, SageMaker Model Monitor will generate recordings of your data and inferences which we can visualize.\n",
    "\n",
    "Here's a function that can help you generate test inputs for your invokations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "#s3.Bucket(bucket).objects.all()\n",
    "#for bucket in s3.Bucket(bucket).objects.limit(5):\n",
    "#    print(bucket.name)\n",
    "#print([obj_summary.key for obj_summary in  s3.Bucket(bucket).objects.all()])\n",
    "#s3.Bucket(bucket).objects.all()\n",
    "\n",
    "\n",
    "objects = s3.Bucket(bucket).objects.filter(prefix=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.collection.s3.Bucket.objectsCollection"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "my_iter_list = iter(objects)\n",
    "\n",
    "print(type(my_iter_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-936e785cbd43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print([obj_summary.key for obj_summary in objects.all()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36mpages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# we start processing and yielding individual items.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mpage_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inject_starting_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, current_kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    690\u001b[0m         }\n\u001b[1;32m    691\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 692\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                  context=None):\n\u001b[1;32m    737\u001b[0m         api_params = self._emit_api_params(\n\u001b[0;32m--> 738\u001b[0;31m             api_params, operation_model, context)\n\u001b[0m\u001b[1;32m    739\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[1;32m    740\u001b[0m             api_params, operation_model)\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_emit_api_params\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    768\u001b[0m                 \u001b[0mservice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                 operation_name=operation_name),\n\u001b[0;32m--> 770\u001b[0;31m             params=api_params, model=operation_model, context=context)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/handlers.py\u001b[0m in \u001b[0;36mvalidate_bucket_name\u001b[0;34m(params, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mVALID_BUCKET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mVALID_S3_ARN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         error_msg = (\n\u001b[1;32m    240\u001b[0m             \u001b[0;34m'Invalid bucket name \"%s\": Bucket name must match '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for b in objects: c+=1\n",
    "c\n",
    "#print([obj_summary.key for obj_summary in objects.all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f0928155e2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"s3_key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     })\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgenerate_test_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-f0928155e2dd>\u001b[0m in \u001b[0;36mgenerate_test_case\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Grab any random object key from that folder!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     return json.dumps({\n",
      "\u001b[0;32m<ipython-input-18-f0928155e2dd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Grab any random object key from that folder!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     return json.dumps({\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/resources/collection.py\u001b[0m in \u001b[0;36mpages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# we start processing and yielding individual items.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mpage_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inject_starting_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, current_kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    690\u001b[0m         }\n\u001b[1;32m    691\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 692\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                  context=None):\n\u001b[1;32m    737\u001b[0m         api_params = self._emit_api_params(\n\u001b[0;32m--> 738\u001b[0;31m             api_params, operation_model, context)\n\u001b[0m\u001b[1;32m    739\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[1;32m    740\u001b[0m             api_params, operation_model)\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_emit_api_params\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    768\u001b[0m                 \u001b[0mservice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                 operation_name=operation_name),\n\u001b[0;32m--> 770\u001b[0;31m             params=api_params, model=operation_model, context=context)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/handlers.py\u001b[0m in \u001b[0;36mvalidate_bucket_name\u001b[0;34m(params, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mVALID_BUCKET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mVALID_S3_ARN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         error_msg = (\n\u001b[1;32m    240\u001b[0m             \u001b[0;34m'Invalid bucket name \"%s\": Bucket name must match '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_test_case():\n",
    "    # Setup s3 in boto3\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    # Randomly pick from sfn or test folders in our bucket\n",
    "    objects = s3.Bucket(bucket).objects.filter(prefix=\"test\")\n",
    "    \n",
    "    # Grab any random object key from that folder!\n",
    "    obj = random.choice([x.key for x in objects])\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"image_data\": \"\",\n",
    "        \"s3_bucket\": bucket,\n",
    "        \"s3_key\": obj\n",
    "    })\n",
    "generate_test_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Step Function dashboard for your new function, you can create new executions and copy in the generated test cases. Do several executions so that you can generate data you can evaluate and visualize.\n",
    "\n",
    "Once you've done several executions, let's visualize the record of our inferences. Pull in the JSONLines data from your inferences like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# In S3 your data will be saved to a datetime-aware path\n",
    "# Find a path related to a datetime you're interested in\n",
    "data_path = ## TODO: fill in the path to your captured data\n",
    "\n",
    "S3Downloader.download(data_path, \"captured_data\")\n",
    "\n",
    "# Feel free to repeat this multiple times and pull in more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in JSONLines format, where multiple valid JSON objects are stacked on top of eachother in a single `jsonl` file. We'll import an open-source library, `jsonlines` that was purpose built for parsing this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the data from each of the source files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List the file names we downloaded\n",
    "file_handles = os.listdir(\"./captured_data\")\n",
    "\n",
    "# Dump all the data into an array\n",
    "json_data = []\n",
    "for jsonl in file_handles:\n",
    "    with jsonlines.open(f\"./captured_data/{jsonl}\") as f:\n",
    "        json_data.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should now be a list of dictionaries, with significant nesting. We'll give you an example of some code that grabs data out of the objects and visualizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how we'll get our data\n",
    "def simple_getter(obj):\n",
    "    inferences = obj[\"captureData\"][\"endpointOutput\"][\"data\"]\n",
    "    timestamp = obj[\"eventMetadata\"][\"inferenceTime\"]\n",
    "    return json.loads(inferences), timestamp\n",
    "\n",
    "simple_getter(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's an example of a visualization you can build with this data. In this last part, you will take some time and build your own - the captured data has the input images, the resulting inferences, and the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the data for the x and y axis\n",
    "x = []\n",
    "y = []\n",
    "for obj in json_data:\n",
    "    inference, timestamp = simple_getter(obj)\n",
    "    \n",
    "    y.append(max(inference))\n",
    "    x.append(timestamp)\n",
    "\n",
    "# Todo: here is an visualization example, take some time to build another visual that helps monitor the result\n",
    "# Plot the data\n",
    "plt.scatter(x, y, c=['r' if k<.94 else 'b' for k in y ])\n",
    "plt.axhline(y=0.94, color='g', linestyle='--')\n",
    "plt.ylim(bottom=.88)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.suptitle(\"Observed Recent Inferences\", size=14)\n",
    "plt.title(\"Pictured with confidence threshold for production use\", size=10)\n",
    "\n",
    "# Give it some pizzaz!\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: build your own visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You've reached the end of the project. In this project you created an event-drivent ML workflow that can be incorporated into the Scones Unlimited production architecture. You used the SageMaker Estimator API to deploy your SageMaker Model and Endpoint, and you used AWS Lambda and Step Functions to orchestrate your ML workflow. Using SageMaker Model Monitor, you instrumented and observed your Endpoint, and at the end of the project you built a visualization to help stakeholders understand the performance of the Endpoint over time. If you're up for it, you can even go further with these stretch goals:\n",
    "\n",
    "* Extend your workflow to incorporate more classes: the CIFAR dataset includes other vehicles that Scones Unlimited can identify with this model.\n",
    "* Modify your event driven workflow: can you rewrite your Lambda functions so that the workflow can process multiple image inputs in parallel? Can the Step Function \"fan out\" to accomodate this new workflow?\n",
    "* Consider the test data generator we provided for you. Can we use it to create a \"dummy data\" generator, to simulate a continuous stream of input data? Or a big paralell load of data?\n",
    "* What if we want to get notified every time our step function errors out? Can we use the Step Functions visual editor in conjunction with a service like SNS to accomplish this? Try it out!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
